# Hypergraph-guided Diffusion 系统设计说明

## 一、方法整体说明

本系统实现了 **Hypergraph-guided Diffusion for Unpaired Virtual Stain Translation**，核心思想是将 **条件扩散模型** 与 **超图结构约束** 相结合，用于无配对病理虚拟染色迁移。

### 核心创新

1. **条件扩散模型（Conditional Diffusion）**
   - 使用 U-Net 作为 backbone
   - 以 source stain 图像为条件，逐步去噪生成 target stain 图像
   - 遵循标准 DDPM 流程

2. **超图结构约束（Hypergraph Structure Constraint）**
   - 从 U-Net 的中间特征层（bottleneck）提取 patch-level features
   - 使用 soft k-means 聚类构建 hyperedges
   - 通过 Hypergraph Neural Network (HGNN) 进行信息传播
   - 约束跨 domain 的 patch-level 结构一致性

3. **无配对训练（Unpaired Training）**
   - 无需像素级配对数据
   - 通过对比学习（InfoNCE）约束结构一致性
   - **不使用** cycle consistency（区别于 CycleGAN）

## 二、系统架构图的文字描述（模块级）

### 整体流程

```
输入: Source Image (Domain A) [B, 3, H, W]
      Target Image (Domain B) [B, 3, H, W]  (unpaired)

┌─────────────────────────────────────────────────────────┐
│ Step 1: Forward Diffusion                               │
│   q(x_t | x_0) = √(α̅_t) x_0 + √(1-α̅_t) ε            │
│   t ~ Uniform(0, T)                                     │
└─────────────────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────────────────┐
│ Step 2: U-Net Forward Pass                              │
│   UNet(x_t, t, condition=source_image)                  │
│   ├── Encoder: 下采样                                    │
│   ├── Bottleneck: 特征提取 ←───┐                        │
│   └── Decoder: 上采样            │                        │
│                                 │                        │
│   ┌─────────────────────────────┘                        │
│   │ Extract Patch Features                               │
│   │ [B, num_patches, feature_dim]                        │
│   └─────────────────────────────────────────────────────┘
└─────────────────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────────────────┐
│ Step 3: Hypergraph Construction                        │
│   ├── Soft K-Means Clustering                          │
│   │   ├── Memberships: [B, num_patches, num_hyperedges]│
│   │   ├── Centers: [B, feature_dim, num_hyperedges]   │
│   │   └── Hyperedge Matrix & Point-Hyperedge Index     │
│   └── HypergraphConv                                    │
│       ├── Node → Hyperedge Aggregation                  │
│       └── Hyperedge → Node Aggregation                  │
└─────────────────────────────────────────────────────────┘
                    ↓
┌─────────────────────────────────────────────────────────┐
│ Step 4: Loss Computation                                │
│   L_total = λ_diff * L_diff + λ_hg * L_hg             │
│                                                          │
│   L_diff = MSE(predicted_noise, true_noise)            │
│   L_hg = InfoNCE(source_features, target_features)     │
└─────────────────────────────────────────────────────────┘
```

### 关键模块

1. **UNet** (`models/unet.py`)
   - 输入: 噪声图像 [B, C, H, W], 时间步 [B], 条件图像 [B, C, H, W]
   - 输出: 预测噪声 [B, C, H, W]
   - 特征提取: `extract_patch_features()` → [B, num_patches, feature_dim]

2. **Diffusion** (`models/diffusion.py`)
   - 前向扩散: `q_sample()` - 添加噪声
   - 反向采样: `p_sample()` - 去噪一步
   - 完整采样: `p_sample_loop()` - 从纯噪声生成图像

3. **Hypergraph Construction** (`hypergraph/construction.py`)
   - `soft_k_means()`: Soft k-means 聚类
   - `construct_hyperedges()`: 构建 hyperedges

4. **HypergraphConv** (`hypergraph/hgnn.py`)
   - Node → Hyperedge: 聚合每个 hyperedge 内的 nodes
   - Hyperedge → Node: 聚合每个 node 所属的 hyperedges

5. **Loss Functions**
   - `DiffusionLoss`: MSE(predicted_noise, true_noise)
   - `HypergraphContrastiveLoss`: InfoNCE 对比损失

## 三、项目目录结构

```
HypergraphDiffusion/
├── models/                    # 模型定义
│   ├── __init__.py
│   ├── unet.py                # Conditional U-Net
│   └── diffusion.py           # Diffusion 调度器
├── hypergraph/                # 超图模块
│   ├── __init__.py
│   ├── construction.py        # 超图构建
│   └── hgnn.py               # Hypergraph Neural Network
├── losses/                    # 损失函数
│   ├── __init__.py
│   ├── diffusion_loss.py     # Diffusion 损失
│   └── hypergraph_loss.py    # 超图对比损失
├── data/                      # 数据加载
│   └── dataset.py            # 无配对数据集
├── train.py                   # 训练脚本
├── config.py                  # 配置文件
├── example_usage.py          # 使用示例
├── test_basic.py             # 基础测试
├── README.md                  # 项目说明
├── ARCHITECTURE.md            # 架构说明
├── SUMMARY.md                 # 总结文档
├── 设计说明.md                # 本文档
└── requirements.txt          # 依赖包
```

## 四、关键代码骨架（PyTorch）

### 1. UNet 特征提取

```python
# models/unet.py
def extract_patch_features(self, x, timestep, condition, num_patches=64):
    """从 bottleneck 提取 patch features"""
    # ... encoder forward ...
    # 提取 bottleneck 特征
    x = self.mid_block1(x, time_emb)
    x = self.mid_attn(x)
    
    # Reshape 为 patches
    B, C, H, W = x.shape
    x = x.permute(0, 2, 3, 1).reshape(B, H*W, C)
    
    # 随机采样
    indices = torch.randperm(H*W)[:num_patches]
    x = x[:, indices, :]
    
    return x  # [B, num_patches, feature_dim]
```

### 2. Hypergraph 构建

```python
# hypergraph/construction.py
def construct_hyperedges(patch_features, num_clusters, threshold=0.15):
    """构建 hyperedges"""
    # Soft k-means 聚类
    memberships, centers = soft_k_means(
        patch_features, n_clusters=num_clusters
    )
    
    # 构建 hyperedge_matrix: [B, num_hyperedges, num_patches]
    # 构建 point_hyperedge_index: [B, num_patches, max_edges_per_point]
    
    return hyperedge_matrix, point_hyperedge_index, centers
```

### 3. Hypergraph Convolution

```python
# hypergraph/hgnn.py
def forward(self, node_features, hyperedge_matrix, ...):
    """Hypergraph 信息传播"""
    # Step 1: Node → Hyperedge
    node_features_for_hyperedges = batched_index_select(
        node_features, hyperedge_matrix
    )
    aggregated_hyperedge_features = node_features_for_hyperedges.sum(dim=-1)
    
    # Step 2: Hyperedge → Node
    hyperedge_features_for_nodes = batched_index_select(
        aggregated_hyperedge_features, point_hyperedge_index
    )
    aggregated_node_features = hyperedge_features_for_nodes.sum(dim=-1)
    
    return updated_node_features
```

### 4. 损失函数

```python
# losses/hypergraph_loss.py
def forward(self, source_features, target_features):
    """InfoNCE 对比损失"""
    # L2 归一化
    source_features = F.normalize(source_features, p=2, dim=-1)
    target_features = F.normalize(target_features, p=2, dim=-1)
    
    # Positive pairs (对应位置)
    pos_sim = torch.einsum('bnc,bnc->bn', [source_features, target_features])
    
    # Negative pairs (所有其他 patches)
    all_sim = torch.mm(source_flat, target_flat.t())
    
    # InfoNCE loss
    logits = torch.cat([pos_sim, all_sim], dim=1) / temperature
    loss = CrossEntropyLoss(logits, labels=0)
    
    return loss
```

## 五、训练流程伪代码

```python
# train.py
for epoch in range(num_epochs):
    for batch (source_images, target_images) in dataloader:
        # 1. Forward Diffusion
        t ~ Uniform(0, T)
        noise ~ N(0, I)
        x_t = diffusion.q_sample(target_images, t, noise)
        
        # 2. Predict Noise
        predicted_noise = unet(x_t, t, condition=source_images)
        
        # 3. Diffusion Loss
        L_diff = MSE(predicted_noise, noise)
        
        # 4. Extract Patch Features
        source_features = unet.extract_patch_features(
            x_t, t, source_images, num_patches=patch_size
        )
        
        # 5. Build Hypergraph
        hyperedge_matrix, point_hyperedge_index, centers = \
            construct_hyperedges(source_features, num_clusters=num_hyperedges)
        
        # 6. Hypergraph Convolution
        source_features_hg = hypergraph_conv(
            source_features, hyperedge_matrix, point_hyperedge_index, centers
        )
        
        # 7. Target Features (用于对比)
        target_features = unet.extract_patch_features(
            target_images, t=0, target_images, num_patches=patch_size
        )
        
        # 8. Hypergraph Contrastive Loss
        L_hg = InfoNCE(source_features_hg, target_features)
        
        # 9. Total Loss
        L_total = λ_diff * L_diff + λ_hg * L_hg
        
        # 10. Backward
        L_total.backward()
        optimizer.step()
```

## 六、后续可扩展点

### 1. Object-level Hypergraph
- 使用预训练的 object detector（如 Mask R-CNN）提取 object-level features
- 构建更高层次的 hypergraph，捕获组织结构的语义信息

### 2. Attention Conditioning
- 在 U-Net 的 cross-attention 层中融入 hypergraph 结构信息
- 让 attention 机制关注 hypergraph 定义的 patch 关系

### 3. Multi-scale Hypergraph
- 在不同分辨率层级（encoder 的不同层）构建多个 hypergraph
- 进行多尺度结构约束，从细粒度到粗粒度

### 4. Adaptive Hyperedge Number
- 根据图像内容（如组织复杂度）自适应调整 hyperedge 数量
- 使用 learnable 的聚类数量

### 5. Domain-specific Hypergraph
- 为 source 和 target domain 分别构建 hypergraph
- 然后进行跨 domain 对齐（cross-domain hypergraph matching）

### 6. 其他扩展
- **Hierarchical Hypergraph**: 构建层次化的 hypergraph 结构
- **Temporal Hypergraph**: 对于视频数据，考虑时间维度的 hypergraph
- **Semi-supervised Learning**: 利用少量配对数据提升性能

## 七、实验设计建议

### Baseline 对比
1. **Baseline 1**: 无 hypergraph 的 diffusion（`hypergraph_loss_weight=0`）
2. **Baseline 2**: 标准 CycleGAN（如果可用）
3. **Baseline 3**: 其他 diffusion-based I2I 方法（如 Palette, SR3）

### Ablation Studies
1. **Hyperedge 数量**: 3, 6, 9, 12, 15
2. **Patch 数量**: 32, 64, 128, 256
3. **Hypergraph Loss Weight**: 0.01, 0.05, 0.1, 0.2, 0.5
4. **Temperature**: 0.05, 0.07, 0.1, 0.15
5. **Feature Extraction Layer**: bottleneck vs. mid-encoder vs. mid-decoder

### 评估指标
- **结构保持**: SSIM, LPIPS, PSNR
- **风格迁移**: FID, KID, IS
- **定性分析**: 可视化对比（结构保持 vs 纹理变化）

## 八、注意事项

1. **内存占用**: 
   - Hypergraph 构建和对比损失可能占用较多内存
   - 建议 batch_size ≤ 4（单卡 GPU，24GB）

2. **训练稳定性**:
   - 建议使用梯度裁剪（`torch.nn.utils.clip_grad_norm_`）
   - 可以先用较小的 `hypergraph_loss_weight`（如 0.01）开始训练

3. **数据准备**:
   - 确保 source 和 target domain 的图像数量足够（建议各 ≥ 1000）
   - 图像尺寸建议 256×256 或 512×512
   - 数据增强：随机翻转、旋转等

4. **超参数调优**:
   - 学习率：建议从 1e-4 开始
   - Diffusion timesteps：1000（训练）vs 50-100（采样）
   - 可以先用较小的模型（base_channels=32）快速验证

## 九、总结

本系统实现了将 **Diffusion-based Image-to-Image Translation** 与 **Patch-level Hypergraph Learning** 相结合的方法，用于无配对虚拟染色迁移。核心创新在于：

1. **条件扩散模型** 进行图像到图像转换
2. **超图结构约束** 保持跨 domain 的结构一致性
3. **无配对训练** 无需像素级配对数据

代码结构清晰、模块解耦，便于实验和扩展。可以直接基于这套代码跑实验、画图、写论文。

---

**设计完成时间**: 2024年

